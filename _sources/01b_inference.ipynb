{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06a5a6c-d36b-42ef-8d07-c4350b87d92e",
   "metadata": {
    "id": "f06a5a6c-d36b-42ef-8d07-c4350b87d92e"
   },
   "source": [
    "# Model inference \n",
    "\n",
    "Inference is the process of making new predictions on unseen data. There are different approaches to carrying out inference which will depend on purpose of the model and how it will be used. Two main approaches to doing inference are:\n",
    "- 'real-time' single item predictions i.e. calling an API to predict a single example\n",
    "- batch inference i.e. running inference against a larger volume of data \n",
    "\n",
    "Since we have a set of data we want to augment with additional machine generated labels we will use the second batch inference approach. Because we are only likely to run this batch prediction process occasionally, for example if we create a better performing model, we won't spend much time worrying about how quick the inference process is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MuVS8Txhp2zj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuVS8Txhp2zj",
    "outputId": "001e3c6f-3fdc-4b3d-f3d8-33472d67470e",
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastai==2.5.2\n",
      "  Downloading fastai-2.5.2-py3-none-any.whl (186 kB)\n",
      "\u001b[?25l\r\n",
      "\u001b[K     |█▊                              | 10 kB 21.2 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███▌                            | 20 kB 7.0 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████▎                          | 30 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████                         | 40 kB 4.9 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████▉                       | 51 kB 2.5 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████▌                     | 61 kB 2.8 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████▎                   | 71 kB 2.8 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████                  | 81 kB 3.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████▉                | 92 kB 3.3 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████▋              | 102 kB 2.7 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████▍            | 112 kB 2.7 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████           | 122 kB 2.7 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████▉         | 133 kB 2.7 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████▋       | 143 kB 2.7 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████████▍     | 153 kB 2.7 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████████▏   | 163 kB 2.7 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████████████  | 174 kB 2.7 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████████████▋| 184 kB 2.7 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████████████| 186 kB 2.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (0.22.2.post1)\n",
      "Requirement already satisfied: torch<1.10,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (1.9.0+cu111)\n",
      "Collecting fastcore<1.4,>=1.3.8\n",
      "  Downloading fastcore-1.3.27-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (1.1.5)\n",
      "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (2.2.4)\n",
      "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (7.1.2)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (21.1.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (3.2.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (21.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (2.23.0)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (1.0.0)\n",
      "Collecting fastdownload<2,>=0.0.5\n",
      "  Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (0.10.0+cu111)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai==2.5.2) (3.13)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress>=0.2.4->fastai==2.5.2) (1.19.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.2) (4.62.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.2) (1.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.2) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.2) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.2) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.2) (3.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.2) (57.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.2) (2.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.2) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.2) (7.4.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5.2) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5.2) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5.2) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5.2) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5.2) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5.2) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5.2) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5.2) (2.10)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5.2) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5.2) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5.2) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5.2) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->fastai==2.5.2) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai==2.5.2) (2018.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai==2.5.2) (1.0.1)\n",
      "Installing collected packages: fastcore, fastdownload, fastai\n",
      "  Attempting uninstall: fastai\n",
      "    Found existing installation: fastai 1.0.61\n",
      "    Uninstalling fastai-1.0.61:\n",
      "      Successfully uninstalled fastai-1.0.61\n",
      "Successfully installed fastai-2.5.2 fastcore-1.3.27 fastdownload-0.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install fastai==2.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae2387-fb70-473e-b59d-7b53c6fb61b6",
   "metadata": {
    "id": "faae2387-fb70-473e-b59d-7b53c6fb61b6"
   },
   "source": [
    "In the previous notebook we saved our model. We can load it using the `load_model` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f14b5-c129-44f3-ad93-02956edddbd0",
   "metadata": {
    "id": "e80f14b5-c129-44f3-ad93-02956edddbd0"
   },
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_BZikRrZkGL8",
   "metadata": {
    "id": "_BZikRrZkGL8"
   },
   "source": [
    "If you don't have a saved model you can grab one by uncomenting this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyZwXW0Ep8RT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyZwXW0Ep8RT",
    "outputId": "fc2c8883-91d8-4361-8baa-925e85f2155f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-02 19:34:35--  https://zenodo.org/record/5245175/files/20210928-model.pkl?download=1\n",
      "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
      "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 158529715 (151M) [application/octet-stream]\n",
      "Saving to: ‘20210928-model.pkl’\n",
      "\n",
      "20210928-model.pkl  100%[===================>] 151.19M  26.5MB/s    in 6.5s    \n",
      "\n",
      "2021-11-02 19:34:43 (23.3 MB/s) - ‘20210928-model.pkl’ saved [158529715/158529715]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O 20210928-model.pkl https://zenodo.org/record/5245175/files/20210928-model.pkl?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sOw2ELgSr1mQ",
   "metadata": {
    "id": "sOw2ELgSr1mQ"
   },
   "outputs": [],
   "source": [
    "learn_class = load_learner(\"20210928-model.pkl\", cpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeadbaa-d3d0-494b-9c03-1362ede7ce80",
   "metadata": {
    "id": "fdeadbaa-d3d0-494b-9c03-1362ede7ce80"
   },
   "source": [
    "## Trying some examples of made up books\n",
    "\n",
    "To start with let's just call the `predict` method on some made up book titles to see if it gives sensible answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40af358d-e884-4d0c-92a5-84bdc547f447",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "40af358d-e884-4d0c-92a5-84bdc547f447",
    "outputId": "23b14560-7138-413c-bba0-fdbc0f99a2b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('Non-fiction', tensor(1), tensor([0.0081, 0.9919]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_class.predict(\"A history of the French Navy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e3f41-0ba5-415f-a1a9-b21a9f969e69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "055e3f41-0ba5-415f-a1a9-b21a9f969e69",
    "outputId": "79fbff00-95ae-49f1-e1ca-b49773160338"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('Non-fiction', tensor(1), tensor([0.4674, 0.5326]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_class.predict(\"Communist Manifesto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a5b37-3ac6-415c-907c-db84d25483cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "c25a5b37-3ac6-415c-907c-db84d25483cb",
    "outputId": "4b814e5a-03f6-4041-b0f0-35fb34edf64d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('Non-fiction', tensor(1), tensor([0.4055, 0.5945]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_class.predict(\"Lord of the Rings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qiiXt2J7uMPW",
   "metadata": {
    "id": "qiiXt2J7uMPW"
   },
   "source": [
    "These seem sensible enough predictions. We can also see what information we get back from the `predict` method. Particularly important to note here is that we get back a tensor containing the confidence for each prediction. We are likely going to want to keep this information alongside our predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b938e2-c13b-4cb6-8f82-5e1865e7cbd9",
   "metadata": {
    "id": "30b938e2-c13b-4cb6-8f82-5e1865e7cbd9"
   },
   "source": [
    "## Predicting against the full BL Microsoft books metadata \n",
    "\n",
    "We are now ready to run predictions against the full collection of metadata which contains all of the titles we want to have genre labels for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-DyLmmjCshYL",
   "metadata": {
    "id": "-DyLmmjCshYL"
   },
   "outputs": [],
   "source": [
    "full_metadata_url = (\n",
    "    \"https://bl.iro.bl.uk/downloads/e4bf0f74-2c64-4322-93c7-0dcc5e5246da?locale=en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zwh0odYWlKF1",
   "metadata": {
    "id": "zwh0odYWlKF1"
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    \"BL record ID\": \"string\",\n",
    "    \"Type of resource\": \"category\",\n",
    "    \"Name\": \"category\",\n",
    "    \"Role\": \"category\",\n",
    "    \"Title\": \"string\",\n",
    "    \"Country of publication\": \"category\",\n",
    "    \"Place of publication\": \"category\",\n",
    "    \"Publisher\": \"category\",\n",
    "    \"Genre\": \"category\",\n",
    "    \"Languages\": \"category\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2qZSJhodsYvS",
   "metadata": {
    "id": "2qZSJhodsYvS"
   },
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(full_metadata_url, low_memory=False, dtype=dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NYVIjm0yuy7y",
   "metadata": {
    "id": "NYVIjm0yuy7y"
   },
   "source": [
    "As a reminder we can check how big this dataset is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dXY2lzrusj3l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXY2lzrusj3l",
    "outputId": "92a9756f-5ea8-4532-b1e4-4714cd428f21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1752078"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fXQzZNlfnzXi",
   "metadata": {
    "id": "fXQzZNlfnzXi"
   },
   "outputs": [],
   "source": [
    "df_full = df_full[df_full.Title.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C_7XHcWsvG0r",
   "metadata": {
    "id": "C_7XHcWsvG0r"
   },
   "source": [
    "### Creating our test data\n",
    "\n",
    "We need to make sure that our data is processed in the same way when we do inference as when we make predictions. For example our text needs to be tokenized in the same way. This is made very easy in fastai because we can use the `test_dl` method. This method knows how to process data for our model. We just need to pass in the relevant column containing our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11NQqnhCyDI9",
   "metadata": {
    "id": "11NQqnhCyDI9"
   },
   "outputs": [],
   "source": [
    "titles = df_full.loc[:, \"Title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C7OWDTwf2RbI",
   "metadata": {
    "id": "C7OWDTwf2RbI"
   },
   "outputs": [],
   "source": [
    "learn_class.dls.num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5a453-87da-422e-a37a-ee94a1139154",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08f5a453-87da-422e-a37a-ee94a1139154",
    "outputId": "10472b25-0712-4a4c-f779-8128b21da37e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30min 24s, sys: 1min 13s, total: 31min 38s\n",
      "Wall time: 30min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data = learn_class.dls.test_dl(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aduxYDNQyCOl",
   "metadata": {
    "id": "aduxYDNQyCOl"
   },
   "source": [
    "Once we have done this we can use the `get_preds` method to run predictions against all of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba3362a-2fab-43f2-84d1-377a305652ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "aba3362a-2fab-43f2-84d1-377a305652ac",
    "outputId": "fda0789f-7a66-4f80-c05b-daa7f1005673"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 53s, sys: 8.72 s, total: 4min 1s\n",
      "Wall time: 17min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = learn_class.get_preds(dl=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GDoq7j3IyJ-f",
   "metadata": {
    "id": "GDoq7j3IyJ-f"
   },
   "source": [
    "You can see that this didn't take too long considering the size of our data. We might want to double check our predictions match the lenght of our original data. If we just call length on `predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409840f-2a61-4197-9e61-5509597adf69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d409840f-2a61-4197-9e61-5509597adf69",
    "outputId": "4320a57c-7a4a-4179-dfc2-5925fa1ddf3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DIOOM7InyZsN",
   "metadata": {
    "id": "DIOOM7InyZsN"
   },
   "source": [
    "You can see we get something back which has `len` 2. Let's have a look at this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3V-nDgiryY8n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3V-nDgiryY8n",
    "outputId": "076a9e12-b743-4772-ba70-d86148245495"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0759, 0.9241],\n",
       "         [0.1282, 0.8718],\n",
       "         [0.9074, 0.0926],\n",
       "         ...,\n",
       "         [0.0986, 0.9014],\n",
       "         [0.0675, 0.9325],\n",
       "         [0.0834, 0.9166]]), None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g_kFCdQGylY3",
   "metadata": {
    "id": "g_kFCdQGylY3"
   },
   "source": [
    "We can see that this is a tuple, with the first element containing the tensor we're interested in. Let's get the length of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c749124-5fb8-46db-9e11-3cf3ea193e01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c749124-5fb8-46db-9e11-3cf3ea193e01",
    "outputId": "874ecaaa-0ba3-480e-eadb-537d91491796"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1752072"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce30ff-b677-4f92-8544-f4bb6d1c1d4e",
   "metadata": {
    "id": "e0ce30ff-b677-4f92-8544-f4bb6d1c1d4e"
   },
   "outputs": [],
   "source": [
    "assert len(predictions[0]) == len(df_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iOJcQeWcyqte",
   "metadata": {
    "id": "iOJcQeWcyqte"
   },
   "source": [
    "Since we only want the first element of our predictions `tuple` let's store it in a new variable `preds_tensor`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819f3dd-5458-457f-b410-a260d329f533",
   "metadata": {
    "id": "e819f3dd-5458-457f-b410-a260d329f533"
   },
   "outputs": [],
   "source": [
    "preds_tensor = predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2a404-c887-4e0d-9b79-3a79d4e10f76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59c2a404-c887-4e0d-9b79-3a79d4e10f76",
    "outputId": "e24f3a30-54c6-405c-ee62-3e98d776f54f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0759, 0.9241])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_tensor[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X8pcu0IdyxmZ",
   "metadata": {
    "id": "X8pcu0IdyxmZ"
   },
   "source": [
    "At the moment we have the probabilities for each label. We can get the vocab from our `dls` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3cd1e-8bb4-4631-ab57-0963a7464c63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85a3cd1e-8bb4-4631-ab57-0963a7464c63",
    "outputId": "e4680aae-e0e2-4107-f81f-bdb244af37fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fiction', 'Non-fiction']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_class.dls.vocab[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hOc03vh8zKUR",
   "metadata": {
    "id": "hOc03vh8zKUR"
   },
   "source": [
    "To make it easier to work with this data let's map our probabilties to this vocab. We'll first store the `argmax` value for each prediction i.e. the index of the max value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5650f7-34e1-477d-a5b7-0b6144b59681",
   "metadata": {
    "id": "ca5650f7-34e1-477d-a5b7-0b6144b59681"
   },
   "outputs": [],
   "source": [
    "df_full[\"predicted_label\"] = preds_tensor.numpy().argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VQ5tjjzRzVcT",
   "metadata": {
    "id": "VQ5tjjzRzVcT"
   },
   "source": [
    "We can then create a dictionary which we can use to map our `1` and `0` labels to the text versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890fab3-c26c-440d-8059-e13c16debd6e",
   "metadata": {
    "id": "6890fab3-c26c-440d-8059-e13c16debd6e"
   },
   "outputs": [],
   "source": [
    "decode = dict(enumerate(learn_class.dls.vocab[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86fbb0-116d-4beb-a7af-08a459da0a31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b86fbb0-116d-4beb-a7af-08a459da0a31",
    "outputId": "ec60104c-3366-4cda-e65b-12c859e40bd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Fiction', 1: 'Non-fiction'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed892ffe-2f8a-47cd-a92a-a1c13719219f",
   "metadata": {
    "id": "ed892ffe-2f8a-47cd-a92a-a1c13719219f"
   },
   "outputs": [],
   "source": [
    "df_full.predicted_label = df_full.predicted_label.replace(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XwvkSEDszb3G",
   "metadata": {
    "id": "XwvkSEDszb3G"
   },
   "source": [
    "We'll create two new variables to store the probabilties for each of our labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9fdef-22b1-4500-992e-71a871c554cd",
   "metadata": {
    "id": "74b9fdef-22b1-4500-992e-71a871c554cd"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411d8d1-c7cb-46b2-9461-eac06102f8a8",
   "metadata": {
    "id": "1411d8d1-c7cb-46b2-9461-eac06102f8a8"
   },
   "outputs": [],
   "source": [
    "fiction_probs, non_fiction_probs = np.hsplit(preds_tensor.numpy(), learn_class.dls.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1902d-ab28-470a-b427-d0ef48820aae",
   "metadata": {
    "id": "3bb1902d-ab28-470a-b427-d0ef48820aae"
   },
   "outputs": [],
   "source": [
    "df_full[\"fiction_probs\"] = fiction_probs\n",
    "df_full[\"non_fiction_probs\"] = non_fiction_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39050fb-9386-4526-98e5-4c86a0351a09",
   "metadata": {
    "id": "d39050fb-9386-4526-98e5-4c86a0351a09"
   },
   "source": [
    "Let's take a quick look at how our new columns look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff663c8-354c-4841-9f53-85870118fe3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3ff663c8-354c-4841-9f53-85870118fe3d",
    "outputId": "2983b537-a8cf-48c9-cdb4-227ff8b89a65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>fiction_probs</th>\n",
       "      <th>non_fiction_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aabc [etc.] Jesus Vocales, eli äänelliset bokstawit Consonantes Luku-merkit</td>\n",
       "      <td>Non-fiction</td>\n",
       "      <td>0.075868</td>\n",
       "      <td>0.924132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A che serve il Papa?</td>\n",
       "      <td>Non-fiction</td>\n",
       "      <td>0.128236</td>\n",
       "      <td>0.871764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A. for Apple [An illustrated alphabet.]</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>0.907428</td>\n",
       "      <td>0.092572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Á Grãa Bretanha</td>\n",
       "      <td>Non-fiction</td>\n",
       "      <td>0.262661</td>\n",
       "      <td>0.737339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A quien me entiende [On the factious spirit of the Mexican press. Signed: Uno de tantos.]</td>\n",
       "      <td>Non-fiction</td>\n",
       "      <td>0.479002</td>\n",
       "      <td>0.520998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                       Title  ... non_fiction_probs\n",
       "0                Aabc [etc.] Jesus Vocales, eli äänelliset bokstawit Consonantes Luku-merkit  ...          0.924132\n",
       "1                                                                       A che serve il Papa?  ...          0.871764\n",
       "2                                                    A. for Apple [An illustrated alphabet.]  ...          0.092572\n",
       "3                                                                            Á Grãa Bretanha  ...          0.737339\n",
       "4  A quien me entiende [On the factious spirit of the Mexican press. Signed: Uno de tantos.]  ...          0.520998\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full[[\"Title\", \"predicted_label\", \"fiction_probs\", \"non_fiction_probs\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53NjNsZkzoCM",
   "metadata": {
    "id": "53NjNsZkzoCM"
   },
   "source": [
    "This looks like a fairly reasonable format for storing our predictions. Let's save as a `json` and `csv` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191b049-adf6-40f6-a92f-074a2422dd73",
   "metadata": {
    "id": "c191b049-adf6-40f6-a92f-074a2422dd73"
   },
   "outputs": [],
   "source": [
    "df_full.to_json(\"bl_books_w_genre.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eacb7ab-fba0-41da-b85d-fbc2309a56b5",
   "metadata": {
    "id": "0eacb7ab-fba0-41da-b85d-fbc2309a56b5"
   },
   "outputs": [],
   "source": [
    "df_full.to_csv(\"bl_books_w_genre.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qh8p-2VTz11t",
   "metadata": {
    "id": "qh8p-2VTz11t"
   },
   "source": [
    "## Conclusion \n",
    "\n",
    "We have now got a full set of predictions that we could work with. We might want to dig into the potential weakness of our model further though and try and improve on this intial model. We'll do that in the next sections. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "01b_inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
