{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08e83b4",
   "metadata": {},
   "source": [
    "# Assessing Where our Model is Going Wrong\n",
    "\n",
    "The model we trained in the previous notebooks had a fairly high f1 score when we tested the model on our validation set, however when we tested it on a new 'test' dataset our performance was lower. In this notebook we'll dig into this is a little more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a38c5f-6311-46d7-bc43-0c50f3cf2397",
   "metadata": {},
   "source": [
    "## Addressing Worries about Domain Shift: Creating Test Data\n",
    "\n",
    "We have previously discussed domain drift but as a reminder this is broadly when the data we make predictions against is different from, or becomes different from the training data. As an example, a model which is used to predict ice-cream sales which had training data from the summer months is likely to do less well (or badly) at predicting ice-cream sales in the winter. \n",
    "\n",
    "In our case the divergence might be more subtle. One potential issue is in the sampling used to generate our initial training data might not be completely random. This could lead to subtle differences in our training data compared to the data we will be predicting against.\n",
    "\n",
    "### Potential Areas of Difference \n",
    "\n",
    "There are a bunch of ways in which the training data we have doesn't match the target full dataset we want to predict on including:\n",
    "\n",
    "#### Language \n",
    "\n",
    "The full British Library Microsoft Books corpus contains a range of different languages. As the Zooniverse annotation task was done by speakers of a subset of languages there might be an over or under-representation of languages in the training data compared to the full data. \n",
    "\n",
    "#### Dates\n",
    "\n",
    "The full dataset covers a broad time period with the tiles likely varying quite a bit from the beginning of this period 1500s, compared to the later period 1800s. If the training data is over-represented by one period we might expect the model to struggle with titles from different time period. \n",
    "\n",
    "#### Difficult titles skipped \n",
    "\n",
    "Genre is not always clear cut and it can be difficult to tell in some cases. The book could contain a mixture that could be reasonably labeled as either fiction or non-fiction for example, a book of poems with significant commentary about the poems. If annotators skipped these examples the training data won't include as many 'hard' examples for the model to learn from. We could also be generous and say if a human expert annotator is struggling to classify the books genre then a machine learning model might also struggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a75e4-5884-4814-8f97-255c334f1ba6",
   "metadata": {},
   "source": [
    "## Creating a New Randomly Sampled Test Dataset \n",
    "\n",
    "After developing a model we wanted to test out how well it performed on completely unseen data that was randomly sampled from the full BL books corpus. To do this we decided to internally do some verification of the results of the model previously trained. We did this by:\n",
    "\n",
    "- sampling *randomly* from the full books corpus\n",
    "- verifying the models predictions \n",
    "\n",
    "All of this was done using a crude Google sheet with a couple of people working through this data. Since the aim was to cover as many titles as possible there was sometimes some work required to translate titles, or look at the digitized copy of the book in the BL catalogue for verification. This results in a new 'test' dataset which we used to evaluate our model against. \n",
    "\n",
    "This section will explore the results with this test data. First we import some packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb078b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9155589-c1ba-4002-9766-718ad6fdf580",
   "metadata": {},
   "source": [
    "We load our initial input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce75228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_csv(\"data/annotations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b8f20-ea94-42bc-9d66-ab3f8ee14378",
   "metadata": {},
   "source": [
    "and our new test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c49b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/test_errors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6dbdc08-0978-44b1-a581-9c932ff3abae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69565c34-d2a4-4abe-aa8a-09fc42547f7c",
   "metadata": {},
   "source": [
    "We do a bit of tidying to make sure the format of the labels matches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63ac2674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Non-fiction', 'Fiction'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.predicted_label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4788d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['non_fiction', 'fiction', 'both', nan], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.true_label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aebc8e2-4d67-4f4c-8ea3-42a722f090bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[df_test[\"true_label\"].isin([\"non_fiction\", \"fiction\"])]\n",
    "df_test.true_label = df_test.true_label.str.lower()\n",
    "df_test.predicted_label = df_test.predicted_label.str.lower()\n",
    "df_test.predicted_label = df_test.predicted_label.str.replace(\"-\", \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159428c-1f41-412d-b79f-b86499e025a2",
   "metadata": {},
   "source": [
    "### Our Test Data\n",
    "Our new test dataframe includes the following columns which are relevant here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcbdcf89-894f-4b89-86a5-a038fcf11d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>fiction_probs</th>\n",
       "      <th>non_fiction_probs</th>\n",
       "      <th>true_label</th>\n",
       "      <th>free text comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>non_fiction</td>\n",
       "      <td>0.020685</td>\n",
       "      <td>0.979315</td>\n",
       "      <td>non_fiction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non_fiction</td>\n",
       "      <td>0.037538</td>\n",
       "      <td>0.962462</td>\n",
       "      <td>non_fiction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non_fiction</td>\n",
       "      <td>0.389111</td>\n",
       "      <td>0.610889</td>\n",
       "      <td>fiction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>non_fiction</td>\n",
       "      <td>0.050611</td>\n",
       "      <td>0.949388</td>\n",
       "      <td>non_fiction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non_fiction</td>\n",
       "      <td>0.087175</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>non_fiction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>fiction</td>\n",
       "      <td>0.927855</td>\n",
       "      <td>0.072145</td>\n",
       "      <td>fiction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>non_fiction</td>\n",
       "      <td>0.354936</td>\n",
       "      <td>0.645064</td>\n",
       "      <td>fiction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>fiction</td>\n",
       "      <td>0.537227</td>\n",
       "      <td>0.462773</td>\n",
       "      <td>non_fiction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>non_fiction</td>\n",
       "      <td>0.087564</td>\n",
       "      <td>0.912436</td>\n",
       "      <td>fiction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>non_fiction</td>\n",
       "      <td>0.068475</td>\n",
       "      <td>0.931525</td>\n",
       "      <td>non_fiction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>850 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted_label  fiction_probs  non_fiction_probs   true_label  \\\n",
       "0       non_fiction       0.020685           0.979315  non_fiction   \n",
       "1       non_fiction       0.037538           0.962462  non_fiction   \n",
       "2       non_fiction       0.389111           0.610889      fiction   \n",
       "3       non_fiction       0.050611           0.949388  non_fiction   \n",
       "4       non_fiction       0.087175           0.912825  non_fiction   \n",
       "..              ...            ...                ...          ...   \n",
       "994         fiction       0.927855           0.072145      fiction   \n",
       "995     non_fiction       0.354936           0.645064      fiction   \n",
       "996         fiction       0.537227           0.462773  non_fiction   \n",
       "997     non_fiction       0.087564           0.912436      fiction   \n",
       "998     non_fiction       0.068475           0.931525  non_fiction   \n",
       "\n",
       "    free text comment  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2                 NaN  \n",
       "3                 NaN  \n",
       "4                 NaN  \n",
       "..                ...  \n",
       "994               NaN  \n",
       "995               NaN  \n",
       "996               NaN  \n",
       "997               NaN  \n",
       "998               NaN  \n",
       "\n",
       "[850 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\n",
    "    [\n",
    "        \"predicted_label\",\n",
    "        \"fiction_probs\",\n",
    "        \"non_fiction_probs\",\n",
    "        \"true_label\",\n",
    "        \"free text comment\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d8fea2-83e9-4e10-9cd8-c3736f43db5f",
   "metadata": {},
   "source": [
    "We see above the `predicted_label` this is the label predicted by our previously trained mode, we also have the probabilities for these predictions. The `true_label` is a *new* human annotation of the correct label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5391dbe-a29f-4407-8281-473cfabac3e2",
   "metadata": {},
   "source": [
    "### Metrics on our Test Data\n",
    "\n",
    "As a start let's see how our model performed on our new test data i.e. how often it's predictions matched the human annotation for that title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e944b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " non_fiction       0.96      0.72      0.82       296\n",
      "     fiction       0.87      0.98      0.92       554\n",
      "\n",
      "    accuracy                           0.89       850\n",
      "   macro avg       0.91      0.85      0.87       850\n",
      "weighted avg       0.90      0.89      0.89       850\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        df_test.true_label.astype(\"category\").cat.codes,\n",
    "        df_test.predicted_label.astype(\"category\").cat.codes,\n",
    "        target_names=df_test.predicted_label.unique(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21498f-26e6-4e97-8cab-13bfeb5e2532",
   "metadata": {},
   "source": [
    "We can notice a few different things here:\n",
    "\n",
    "- the f1-score is lower than it was in our previous notebook when we looked at the validation data score. \n",
    "- there is a fairly big difference in performance between fiction and non-fiction\n",
    "- the distribution of our labels is still uneven\n",
    "\n",
    "This is already useful for us. We might decide to try and annotate more fiction examples to try and improve the performance or to train our model differently to account for the uneven distribution of labels. Before we jump to doing any of these things let's see if we can understand where our models is going wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efdfe0-686c-4219-9c51-459d469ab3be",
   "metadata": {},
   "source": [
    "## 🤡 Where (and why?) our model sucks?\n",
    "\n",
    "Our single number metrics can be useful for getting an overview of performance on our data as a whole but we sometimes want to also dig into subsets of our data to see whether there are particular facets the model struggles on. \n",
    "\n",
    "### How Does Confidence Vary?\n",
    "\n",
    "One thing we might want to look at is whether the confidence of the predictions has any impact on errors i.e. if the model is confident is it less often wrong compared to when it is less confident?\n",
    "\n",
    "Let's create a new column `argmax` which stores the probability of the predicted label (as a reminder this is the `max` prediction of the two possible labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4add522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"argmax\"] = df_test[[\"fiction_probs\", \"non_fiction_probs\"]].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95749387-2cb8-4e28-b9bb-05c573cc1718",
   "metadata": {},
   "source": [
    "We can quickly take a look at the distribution of the confidence for the predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53c119eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    850.000000\n",
       "mean       0.912980\n",
       "std        0.116743\n",
       "min        0.502686\n",
       "25%        0.904535\n",
       "50%        0.966442\n",
       "75%        0.982130\n",
       "max        0.997301\n",
       "Name: argmax, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"argmax\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde5cdbb-6bef-4383-8264-0821e8fb8f30",
   "metadata": {},
   "source": [
    "We can see here that most of the predictions have above 90% confidence, and 50% of our predictions have above 96% confidence. This is expected because we are using a softmax function when we use cross entropy loss. The lowest predicted confidence is just above 50% suggesting the model was pretty unsure in this example. To make this a bit easier to assess we'll create a new column `correct` which contains whether our models prediction matched the human prediction in our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72b6989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"correct\"] = df_test.true_label == df_test.predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77c992a0-bec9-446a-b4a4-58edb194ea57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2    False\n",
       "Name: correct, dtype: bool"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"correct\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f12e7d-ce72-4104-8a0e-d371d5ffcc97",
   "metadata": {},
   "source": [
    "We can quickly grab some examples where the model was not correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aacb9622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Colville of the Guards']</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>['Metempsychosis. A poem, in two parts. By A. ...</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>['The Cunning-Man ... Originally written and c...</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>['Sheilah McLeod, etc']</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>['Poppæa']</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>['Poems and Imitations of the British Poets, w...</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>['Ralph Royster Doyster, a comedy [in five act...</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>['Helga: a poem in seven Cantos. [With notes a...</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>['The Geological Observer']</td>\n",
       "      <td>non_fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>['Lays of Far Cathay and others. A collection ...</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title   true_label\n",
       "2                           ['Colville of the Guards']      fiction\n",
       "13   ['Metempsychosis. A poem, in two parts. By A. ...      fiction\n",
       "34   ['The Cunning-Man ... Originally written and c...      fiction\n",
       "55                             ['Sheilah McLeod, etc']      fiction\n",
       "63                                          ['Poppæa']      fiction\n",
       "..                                                 ...          ...\n",
       "989  ['Poems and Imitations of the British Poets, w...      fiction\n",
       "993  ['Ralph Royster Doyster, a comedy [in five act...      fiction\n",
       "995  ['Helga: a poem in seven Cantos. [With notes a...      fiction\n",
       "996                        ['The Geological Observer']  non_fiction\n",
       "997  ['Lays of Far Cathay and others. A collection ...      fiction\n",
       "\n",
       "[91 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[df_test[\"correct\"] == False][[\"title\", \"true_label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1252ef5a",
   "metadata": {},
   "source": [
    "## Looking at the Confidence of our Predictions \n",
    "\n",
    "Our model (and most machine learning models) give as output \"logits\" these are the 'raw' prediction. Often we then use a softmax or similar funciton to turn these into a probabiliy distribution. We often just take the argmax of this probabiliy to choose a label i.e. pick the label which the model is most 'confident' about.\n",
    "\n",
    "We might want to use the values associated with these probabilites in some way to determine whether to accept a label. For example if our model gives the following confidences:\n",
    "\n",
    "- fiction: 51%\n",
    "- non_fiction: 49%\n",
    "\n",
    "We could take the argmax value and use 'fiction' as our label. It seems likely however that the model isn't very sure here. In practice we tend to not get very close values like this so often because of the properties of the softmax function but we may still have some labels predicted much more confidently than others.\n",
    "\n",
    "We'll group our date by whether the prediction was correct or not, and the type of label, and then look at the distribution of the `argmax` value (i.e. the probabily of the label which was predicted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e0bd4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct</th>\n",
       "      <th>true_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>fiction</th>\n",
       "      <td>82.0</td>\n",
       "      <td>0.755182</td>\n",
       "      <td>0.148684</td>\n",
       "      <td>0.506073</td>\n",
       "      <td>0.612328</td>\n",
       "      <td>0.794254</td>\n",
       "      <td>0.891587</td>\n",
       "      <td>0.970296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_fiction</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.747011</td>\n",
       "      <td>0.172843</td>\n",
       "      <td>0.537227</td>\n",
       "      <td>0.624419</td>\n",
       "      <td>0.707089</td>\n",
       "      <td>0.899563</td>\n",
       "      <td>0.989327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th>fiction</th>\n",
       "      <td>214.0</td>\n",
       "      <td>0.869542</td>\n",
       "      <td>0.134010</td>\n",
       "      <td>0.502686</td>\n",
       "      <td>0.808385</td>\n",
       "      <td>0.924449</td>\n",
       "      <td>0.974263</td>\n",
       "      <td>0.996350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_fiction</th>\n",
       "      <td>545.0</td>\n",
       "      <td>0.956519</td>\n",
       "      <td>0.060473</td>\n",
       "      <td>0.541530</td>\n",
       "      <td>0.957278</td>\n",
       "      <td>0.974239</td>\n",
       "      <td>0.983971</td>\n",
       "      <td>0.997301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count      mean       std       min       25%       50%  \\\n",
       "correct true_label                                                             \n",
       "False   fiction       82.0  0.755182  0.148684  0.506073  0.612328  0.794254   \n",
       "        non_fiction    9.0  0.747011  0.172843  0.537227  0.624419  0.707089   \n",
       "True    fiction      214.0  0.869542  0.134010  0.502686  0.808385  0.924449   \n",
       "        non_fiction  545.0  0.956519  0.060473  0.541530  0.957278  0.974239   \n",
       "\n",
       "                          75%       max  \n",
       "correct true_label                       \n",
       "False   fiction      0.891587  0.970296  \n",
       "        non_fiction  0.899563  0.989327  \n",
       "True    fiction      0.974263  0.996350  \n",
       "        non_fiction  0.983971  0.997301  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby([\"correct\", \"true_label\"])[\"argmax\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad82e67",
   "metadata": {},
   "source": [
    "Let's start with the 'mean' `argmax` value. We can see that the when the prediction was correct for fiction, the mean argmax value was `0.95`. When the model incorectly predicted 'fiction' the mean argmax value was `0.75`. \n",
    "\n",
    "When the model correctly predicted 'non_fiction' it had a mean `argmax` value of `0.95`. When the model incorrectly predicted the label 'non_fiction' it had a mean `argmax` value of `0.74`.\n",
    "\n",
    "This possibly suggests that we might want to set a 'threshold' for when we accept the model's predictions for each label. There is a trade off here. If we increase the threshold higher there will be more labels where the model's suggestions are ignored. If we set the threshold too low we might get more mistakes. Let's see what happens to our models performance if we set some thresholds based on the above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cb8ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_test.loc[\n",
    "    ((df_test.predicted_label == \"fiction\") & (df_test.fiction_probs > 0.90))\n",
    "    | ((df_test.predicted_label == \"non_fiction\") & (df_test.non_fiction_probs > 0.97))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45e58ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " non_fiction       0.98      0.99      0.99       125\n",
      "     fiction       1.00      0.99      1.00       326\n",
      "\n",
      "    accuracy                           0.99       451\n",
      "   macro avg       0.99      0.99      0.99       451\n",
      "weighted avg       0.99      0.99      0.99       451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        df_subset.true_label,\n",
    "        df_subset.predicted_label,\n",
    "        target_names=df_subset.predicted_label.unique(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684f0d3",
   "metadata": {},
   "source": [
    "We can see if we pick these higher thresholds, our model does much better. This comes at the expense of coverage. Working out whether you prefer an accurate model or a model that labels all of your data depends in a large part to how you are using your models predictions. You may decide for example to get humans to annotate the examples your model struggled with. If you are working at a very large scale and the alternative is no labels for genre you might still prefer a noisy label compared to no label at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c4f28-722d-49ba-8a8a-4c44a88bdabd",
   "metadata": {},
   "source": [
    "## Other Factors That Impact Model Performance?\n",
    "\n",
    "There may be other types of title where our model is wrong more often. We can draw on our intuitions to some extent here but one things that can be very helpful is to have annotated some of the data yourself. If you completely outsource the process of creating training you might not have seen enough examples of the data to have any sense of what it looks like. In the process of creating the test data we noticed some titles are very short and others very long. If a title is very short it has less information and it's possible our model will struggle. Let's see if this is the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40c7e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"title_length\"] = df_test.title.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a18d38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct</th>\n",
       "      <th>true_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>fiction</th>\n",
       "      <td>82.0</td>\n",
       "      <td>92.865854</td>\n",
       "      <td>59.902341</td>\n",
       "      <td>10.0</td>\n",
       "      <td>54.75</td>\n",
       "      <td>80.5</td>\n",
       "      <td>119.0</td>\n",
       "      <td>301.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_fiction</th>\n",
       "      <td>9.0</td>\n",
       "      <td>50.111111</td>\n",
       "      <td>22.273552</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>44.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th>fiction</th>\n",
       "      <td>214.0</td>\n",
       "      <td>50.714953</td>\n",
       "      <td>29.242680</td>\n",
       "      <td>15.0</td>\n",
       "      <td>31.00</td>\n",
       "      <td>41.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>179.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_fiction</th>\n",
       "      <td>545.0</td>\n",
       "      <td>118.253211</td>\n",
       "      <td>73.611723</td>\n",
       "      <td>9.0</td>\n",
       "      <td>66.00</td>\n",
       "      <td>102.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count        mean        std   min    25%    50%    75%  \\\n",
       "correct true_label                                                             \n",
       "False   fiction       82.0   92.865854  59.902341  10.0  54.75   80.5  119.0   \n",
       "        non_fiction    9.0   50.111111  22.273552  27.0  29.00   44.0   64.0   \n",
       "True    fiction      214.0   50.714953  29.242680  15.0  31.00   41.0   61.0   \n",
       "        non_fiction  545.0  118.253211  73.611723   9.0  66.00  102.0  144.0   \n",
       "\n",
       "                       max  \n",
       "correct true_label          \n",
       "False   fiction      301.0  \n",
       "        non_fiction   93.0  \n",
       "True    fiction      179.0  \n",
       "        non_fiction  499.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby([\"correct\", \"true_label\"])[\"title_length\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a51d7-47ce-4d03-800f-0a93f2cfb50e",
   "metadata": {},
   "source": [
    "We can see here that when the model predicts fiction the mean length of the title when the model was incorrect is `~92`, when a fiction prediction is correct the man length of the title `~50`. This suggests that maybe our model actually gets thrown off by longer fiction book titles. We see the opposite with non-fiction where our model seems to correctly prediction non-fiction titles when these titles are longer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537f33b-a42b-4c3d-8b4d-3674d02e6b1b",
   "metadata": {},
   "source": [
    "## Looking at Examples\n",
    "\n",
    "We can look at some examples of titles to get a better intuition for how our model is behaving. Let's start with incorrect predictions, where the model was confident. We'll look at 10 of the most confident wrong examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a64f400-1e8b-4b21-be76-218d4670930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction 0.98932695\n",
      "['Janet Delille. [A novel.]']\n",
      "--------------------------------------------------------------------------------\n",
      "non_fiction 0.97029626\n",
      "['Royston Winter Recreations in the days of Queen Anne: translated into Spenserian stanza by the Rev. W. W. Harvey, ... from a contemporary Latin poem [entitled “Bruma,” etc.] ... With illustrations by H. J. Thurnall, and notes on Royston Memorabilia by the Royston Publisher [J. Warren]']\n",
      "--------------------------------------------------------------------------------\n",
      "non_fiction 0.96185756\n",
      "['Legends of the Afghan Countries. In verse. With various pieces, original and translated']\n",
      "--------------------------------------------------------------------------------\n",
      "non_fiction 0.9605836\n",
      "['In Cornwall, and Across the Sea [in verse]; with poems written in Devonshire, etc']\n",
      "--------------------------------------------------------------------------------\n",
      "non_fiction 0.9512329\n",
      "['[Pierre le Grand, comédie en quatre actes, et en prose mêlée de chants, etc.]']\n",
      "--------------------------------------------------------------------------------\n",
      "non_fiction 0.9509604\n",
      "['The poetical works of Oliver Goldsmith. With remarks, attempting to ascertain ... the actual scene of the deserted village; and illustrative engravings, by Mr. Alkin, from drawings taken upon the spot. By the Rev. R. H. Newell', 'Collections. III. Poems']\n",
      "--------------------------------------------------------------------------------\n",
      "non_fiction 0.9478657\n",
      "['The City of Dreadful Night. And other stories. [“With the Calcutta Police” from “The City of Dreadful Night” and other selected tales. With “Character Sketch of Rudyard Kipling. By Rev. C. O. Day.”]', 'Selections']\n",
      "--------------------------------------------------------------------------------\n",
      "fiction 0.9427181\n",
      "['The Lost Manuscripts of a Blue Jacket']\n",
      "--------------------------------------------------------------------------------\n",
      "non_fiction 0.93851966\n",
      "['[Envy at arms! or, Caloric alarming the Church. [A satire in verse by - Thom? occasioned by the opposition of the Ministers of Edinburgh to the election of J. Leslie to the professorship of mathematics in the University of Edinburgh.]]']\n",
      "--------------------------------------------------------------------------------\n",
      "non_fiction 0.9349319\n",
      "['Select British Poets, or new elegant extracts from Chaucer to the present time, with critical remarks. By W. Hazlitt', 'Single Works']\n",
      "--------------------------------------------------------------------------------\n",
      "non_fiction 0.9300464\n",
      "['The Select Poetical Works of Sir Walter Scott; comprising The Lay of the Last Minstrel; Marmion; The Lady of the Lake; ballads, lyrical pieces, etc. MS. notes', 'Collections of Works. Smaller Collections of Poems']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(\n",
    "    df_test[df_test[\"correct\"] == False]\n",
    "    .sort_values(\"argmax\", ascending=False)\n",
    "    .itertuples()\n",
    "):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(row.predicted_label, row.argmax)\n",
    "    print(row.title)\n",
    "    print(\"----\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27039af-45be-474c-8c7e-8c2c8c224660",
   "metadata": {},
   "source": [
    "We can do the same for examples where our model is correct but also was not confident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a36657c-9293-43db-8831-568ac0468acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction 0.50268555\n",
      "['A Legend of Fyvie Castle. By K. G. [i.e. Catherine J. B. Gordon.]']\n",
      "--------------------------------------------------------------------------------\n",
      "fiction 0.50299066\n",
      "['Heera, the Maid of the Dekhan. A poem, in five cantos']\n",
      "--------------------------------------------------------------------------------\n",
      "fiction 0.5074\n",
      "['Rambling Rhymes']\n",
      "--------------------------------------------------------------------------------\n",
      "fiction 0.5121436\n",
      "['The Small House at Allington. With eighteen illustrations by J. E. Millais', 'Single Works']\n",
      "--------------------------------------------------------------------------------\n",
      "fiction 0.5274991\n",
      "['Abd-el-Kader: a poem, in six cantos']\n",
      "--------------------------------------------------------------------------------\n",
      "fiction 0.528868\n",
      "['A Cure for the Heart-ache; a comedy, in five acts [in prose], etc']\n",
      "--------------------------------------------------------------------------------\n",
      "fiction 0.53044933\n",
      "[\"Auld Lang Syne. By the author of “The Wreck of the 'Grosvenor”' [i.e. William Clark Russell]\"]\n",
      "--------------------------------------------------------------------------------\n",
      "non_fiction 0.5415301\n",
      "['Wreck of the “London.” [With illustrations.]']\n",
      "--------------------------------------------------------------------------------\n",
      "fiction 0.5452555\n",
      "['The State-farce: a lyrick. Written at Clermont, and inscribed to His Grace the Duke of Newcastle']\n",
      "--------------------------------------------------------------------------------\n",
      "fiction 0.5558798\n",
      "['Miscellaneous Poems']\n",
      "--------------------------------------------------------------------------------\n",
      "fiction 0.5597714\n",
      "[\"[The Actress's Ways and Means to industriously raise the Wind! Containing the moral and entertaining poetical effusions of Mrs. R. Beverley.]\"]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(\n",
    "    df_test[df_test[\"correct\"] == True]\n",
    "    .sort_values(\"argmax\", ascending=True)\n",
    "    .itertuples()\n",
    "):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(row.predicted_label, row.argmax)\n",
    "    print(row.title)\n",
    "    print(\"----\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c9291-8a93-4fc2-a11f-28503227ba2e",
   "metadata": {},
   "source": [
    "### Things we Might Notice\n",
    "\n",
    "- There might be some examples which are incorrectly labeled or are very hard to tell\n",
    "- There might be particular title phrases such as `[With Illustrations]` that might throw our model off\n",
    "- Long titles with lots of proper nouns might confuse our model?\n",
    "- ???\n",
    "\n",
    "We would need to do some more digging to see if there are other patterns in the errors of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d98eb9-5e4c-449e-8159-09933ad9eeac",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "We have seen that if we bump up the threshold for the models confidence it improves quite a bit. This could be one way of dealing with the outputs of this model in a pragmatic way. It's not always possible to train a perfect model but if we have a model that does quite well and we can also use only some of its outputs we might still be able to do a lot more than we could without machine learning. This could be particularly useful when we use outputs of external models that other people have trained since we might have less ability to control the training process but can still set thresholds of when to accept predictions or not. \n",
    "\n",
    "We'll now move on to trying to improve our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7037df69-b206-4007-b99a-fd95eb6029f2",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "The main things we tried to show in this notebook:\n",
    "- It is very helpful to have true test data to get a proper sense of how well your model will perform on unseen data\n",
    "- The models performance was uneven across our labels\n",
    "- There might be some types of title that our model struggles with more \n",
    "- We can decide a threshold for when we accept our models predictions this can often improve the performance of our model\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
