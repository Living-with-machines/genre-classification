
@article{devlinBERTPretrainingDeep2019,
  title         = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle    = {{{BERT}}},
  author        = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year          = {2019},
  month         = may,
  journal       = {arXiv:1810.04805 [cs]},
  eprint        = {1810.04805},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  abstract      = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language},
  file          = {/Users/dvanstrien/Zotero/storage/EGCEZC7F/Devlin et al_2019_BERT.pdf;/Users/dvanstrien/Zotero/storage/N8RQY97T/1810.html}
}

@misc{EfficientMultilingualLanguage,
  title        = {Efficient Multi-Lingual Language Model Fine-Tuning {$\cdot$} Fast.Ai {{NLP}}},
  howpublished = {https://nlp.fast.ai/classification/2019/09/10/multifit.html},
  file         = {/Users/dvanstrien/Zotero/storage/ST2HMEVW/multifit.html}
}

@article{eisenschlosMultiFiTEfficientMultilingual2020,
  title         = {{{MultiFiT}}: {{Efficient Multi}}-Lingual {{Language Model Fine}}-Tuning},
  shorttitle    = {{{MultiFiT}}},
  author        = {Eisenschlos, Julian Martin and Ruder, Sebastian and Czapla, Piotr and Kardas, Marcin and Gugger, Sylvain and Howard, Jeremy},
  year          = {2020},
  month         = jun,
  journal       = {arXiv:1909.04761 [cs]},
  eprint        = {1909.04761},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  abstract      = {Pretrained language models are promising particularly for low-resource languages as they only require unlabelled data. However, training existing models requires huge amounts of compute, while pretrained cross-lingual models often underperform on low-resource languages. We propose Multi-lingual language model Fine-Tuning (MultiFiT) to enable practitioners to train and fine-tune language models efficiently in their own language. In addition, we propose a zero-shot method using an existing pretrained cross-lingual model. We evaluate our methods on two widely used cross-lingual classification datasets where they outperform models pretrained on orders of magnitude more data and compute. We release all models and code.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file          = {/Users/dvanstrien/Zotero/storage/DJMBWAUP/Eisenschlos et al_2020_MultiFiT.pdf;/Users/dvanstrien/Zotero/storage/9AQ5JBGP/1909.html}
}

@article{howardFastaiLayeredAPI2020,
  title         = {Fastai: {{A Layered API}} for {{Deep Learning}}},
  shorttitle    = {Fastai},
  author        = {Howard, Jeremy and Gugger, Sylvain},
  year          = {2020},
  month         = feb,
  journal       = {Information},
  volume        = {11},
  number        = {2},
  eprint        = {2002.04688},
  eprinttype    = {arxiv},
  pages         = {108},
  issn          = {2078-2489},
  doi           = {10.3390/info11020108},
  abstract      = {fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We have used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching. NB: This paper covers fastai v2, which is currently in pre-release at http://dev.fast.ai/},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file          = {/Users/dvanstrien/Zotero/storage/DJM5RW46/Howard_Gugger_2020_fastai.pdf;/Users/dvanstrien/Zotero/storage/EZNFDQZA/2002.html}
}

@article{howardUniversalLanguageModel2018,
  title         = {Universal {{Language Model Fine}}-Tuning for {{Text Classification}}},
  author        = {Howard, Jeremy and Ruder, Sebastian},
  year          = {2018},
  month         = may,
  journal       = {arXiv:1801.06146 [cs, stat]},
  eprint        = {1801.06146},
  eprinttype    = {arxiv},
  primaryclass  = {cs, stat},
  abstract      = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file          = {/Users/dvanstrien/Zotero/storage/F8FKKPCH/Howard_Ruder_2018_Universal Language Model Fine-tuning for Text Classification.pdf;/Users/dvanstrien/Zotero/storage/9CRUAVGV/1801.html}
}

@article{iwanaJudgingBookIts2017,
  title         = {Judging a {{Book By}} Its {{Cover}}},
  author        = {Iwana, Brian Kenji and Rizvi, Syed Tahseen Raza and Ahmed, Sheraz and Dengel, Andreas and Uchida, Seiichi},
  year          = {2017},
  month         = oct,
  journal       = {arXiv:1610.09204 [cs]},
  eprint        = {1610.09204},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  abstract      = {Book covers communicate information to potential readers, but can that same information be learned by computers? We propose using a deep Convolutional Neural Network (CNN) to predict the genre of a book based on the visual clues provided by its cover. The purpose of this research is to investigate whether relationships between books and their covers can be learned. However, determining the genre of a book is a difficult task because covers can be ambiguous and genres can be overarching. Despite this, we show that a CNN can extract features and learn underlying design rules set by the designer to define a genre. Using machine learning, we can bring the large amount of resources available to the book cover design process. In addition, we present a new challenging dataset that can be used for many pattern recognition tasks.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  file          = {/Users/dvanstrien/Zotero/storage/3LX7I5ZZ/Iwana et al. - 2017 - Judging a Book By its Cover.pdf;/Users/dvanstrien/Zotero/storage/KRUAWWY5/1610.html}
}

@article{lorenziniAutomaticallyEvaluatingQuality2021a,
  title    = {Automatically Evaluating the Quality of Textual Descriptions in Cultural Heritage Records},
  author   = {Lorenzini, Matteo and Rospocher, Marco and Tonelli, Sara},
  year     = {2021},
  month    = jun,
  journal  = {International Journal on Digital Libraries},
  volume   = {22},
  number   = {2},
  pages    = {217--231},
  issn     = {1432-1300},
  doi      = {10.1007/s00799-021-00302-1},
  abstract = {Metadata are fundamental for the indexing, browsing and retrieval of cultural heritage resources in repositories, digital libraries and catalogues. In order to be effectively exploited, metadata information has to meet some quality standards, typically defined in the collection usage guidelines. As manually checking the quality of metadata in a repository may not be affordable, especially in large collections, in this paper we specifically address the problem of automatically assessing the quality of metadata, focusing in particular on textual descriptions of cultural heritage items. We describe a novel approach based on machine learning that tackles this problem by framing it as a binary text classification task aimed at evaluating the accuracy of textual descriptions. We report our assessment of different classifiers using a new dataset that we developed, containing more than 100K descriptions. The dataset was extracted from different collections and domains from the Italian digital library ``Cultura Italia'' and was annotated with accuracy information in terms of compliance with the cataloguing guidelines. The results empirically confirm that our proposed approach can effectively support curators (F1 \$\$\textbackslash sim \$\$~0.85) in assessing the quality of the textual descriptions of the records in their collections and provide some insights into how training data, specifically their size and domain, can affect classification performance.},
  language = {en},
  file     = {/Users/dvanstrien/Zotero/storage/EA5IIMBA/Lorenzini et al_2021_Automatically evaluating the quality of textual descriptions in cultural.pdf}
}

@article{merityRegularizingOptimizingLSTM2017,
  title         = {Regularizing and {{Optimizing LSTM Language Models}}},
  author        = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  year          = {2017},
  month         = aug,
  journal       = {arXiv:1708.02182 [cs]},
  eprint        = {1708.02182},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  abstract      = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTMbased models. We propose the weight-dropped LSTM which uses DropConnect on hidden-tohidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
  archiveprefix = {arXiv},
  language      = {en},
  keywords      = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file          = {/Users/dvanstrien/Zotero/storage/ZY296W22/Merity et al. - 2017 - Regularizing and Optimizing LSTM Language Models.pdf}
}

@article{meritySingleHeadedAttention2019,
  title         = {Single {{Headed Attention RNN}}: {{Stop Thinking With Your Head}}},
  shorttitle    = {Single {{Headed Attention RNN}}},
  author        = {Merity, Stephen},
  year          = {2019},
  month         = nov,
  journal       = {arXiv:1911.11423 [cs]},
  eprint        = {1911.11423},
  eprinttype    = {arxiv},
  primaryclass  = {cs},
  abstract      = {The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file          = {/Users/dvanstrien/Zotero/storage/ZZ35UG7S/Merity_2019_Single Headed Attention RNN.pdf;/Users/dvanstrien/Zotero/storage/JA6THSTZ/1911.html}
}

@inproceedings{ozsarfatiBookGenreClassification2019,
  title     = {Book {{Genre Classification Based}} on {{Titles}} with {{Comparative Machine Learning Algorithms}}},
  booktitle = {2019 {{IEEE}} 4th {{International Conference}} on {{Computer}} and {{Communication Systems}} ({{ICCCS}})},
  author    = {Ozsarfati, Eran and Sahin, Egemen and Saul, Can Jozef and Yilmaz, Alper},
  year      = {2019},
  month     = feb,
  pages     = {14--20},
  doi       = {10.1109/CCOMS.2019.8821643},
  abstract  = {This paper presents algorithmic comparisons for producing a book's genre based on its title. While some titles are easy to interpret, some are irrelevant to the genre that they belong to. Henceforth, we seek to determine the optimal and most accurate method for accomplishing the task. Several data preprocessing steps were implemented, in which word embeddings were created to make the titles operable by the computer. Five different machine learning models were tested throughout the experiment. Each different algorithm was fine-tuned for attaining the best parameter values, while no modifications were conducted on the dataset. The results indicate that the Long Short-Term Memory (LSTM) with a dropout is the top performing architecture among the algorithms, with an accuracy of 65.58\%. To the authors' knowledge, no prior study has been done about book genre classification by title, therefore the present study is the current best in the field.},
  keywords  = {Book Title,Computer architecture,Deep learning,Deep Learning,Genre Classification,Logic gates,Long Short-Term Memory,Machine Learning,Machine learning algorithms,Natural Language Processing,Recurrent neural networks,Task analysis},
  file      = {/Users/dvanstrien/Zotero/storage/HS9EMTBG/Ozsarfati et al_2019_Book Genre Classification Based on Titles with Comparative Machine Learning.pdf;/Users/dvanstrien/Zotero/storage/IHE5Q8PS/8821643.html}
}

@misc{thomasHowWhyCreate,
  title        = {How (and Why) to Create a Good Validation Set {$\cdot$} Fast.Ai},
  author       = {Thomas, Rachel},
  journal      = {fast.ai},
  howpublished = {https://www.fast.ai/2017/11/13/validation-sets/},
  file         = {/Users/dvanstrien/Zotero/storage/MK3HU8WX/validation-sets.html}
}

@article{mitchell2019,
  title     = {Model Cards for Model Reporting},
  url       = {http://dx.doi.org/10.1145/3287560.3287596},
  doi       = {10.1145/3287560.3287596},
  journal   = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  publisher = {ACM},
  author    = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  year      = {2019},
  month     = {Jan}
}

@misc{gebru2020datasheets,
  title         = {Datasheets for Datasets},
  author        = {Timnit Gebru and Jamie Morgenstern and Briana Vecchione and Jennifer Wortman Vaughan and Hanna Wallach and Hal Daumé III au2 and Kate Crawford},
  year          = {2020},
  eprint        = {1803.09010},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DB}
}

@misc{devlin2019bert,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{vaswani2017attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2017},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{osma_suominen_2021_5654173,
  author    = {Osma Suominen and
               Juho Inkinen and
               Tuomo Virolainen and
               Moritz Fürneisen and
               Bruno P. Kinoshita and
               Sara Veldhoen and
               Mats Sjöberg and
               Philipp Zumstein and
               Robin Neatherway and
               monalehtinen},
  title     = {NatLibFi/Annif: Annif 0.55},
  month     = nov,
  year      = 2021,
  publisher = {Zenodo},
  version   = {v0.55.0},
  doi       = {10.5281/zenodo.5654173},
  url       = {https://doi.org/10.5281/zenodo.5654173}
}
@misc{marx_2021,
  title   = {Theses on Feuerbach},
  url     = {https://www.marxists.org/archive/marx/works/1845/theses/theses.htm},
  journal = {Marxists.org},
  author  = {Marx, Karl},
  year    = {2021}
}

@article{lorenzini_rospocher_tonelli_2021,
  title   = {Automatically evaluating the quality of textual descriptions in cultural heritage records},
  volume  = {22},
  doi     = {10.1007/s00799-021-00302-1},
  number  = {2},
  journal = {International Journal on Digital Libraries},
  author  = {Lorenzini, Matteo and Rospocher, Marco and Tonelli, Sara},
  year    = {2021},
  pages   = {217-231}
}

@article{morris2020,
  author    = {Victoria Morris},
  title     = {Automated Language Identification of Bibliographic Resources},
  journal   = {Cataloging \& Classification Quarterly},
  volume    = {58},
  number    = {1},
  pages     = {1-27},
  year      = {2020},
  publisher = {Routledge},
  doi       = {10.1080/01639374.2019.1700201},
  url       = {https://doi.org/10.1080/01639374.2019.1700201},
  eprint    = {https://doi.org/10.1080/01639374.2019.1700201}
}

@inproceedings{NIPS2016_6709e8d6,
  author    = {Ratner, Alexander J and De Sa, Christopher M and Wu, Sen and Selsam, Daniel and R\'{e}, Christopher},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Data Programming: Creating Large Training Sets, Quickly},
  url       = {https://proceedings.neurips.cc/paper/2016/file/6709e8d64a5f47269ed5cea9f625f7ab-Paper.pdf},
  volume    = {29},
  year      = {2016}
}
